=======================
### Resolutions

Different resolutions can be asked for.

For a 16U image,
Threshold computation is always the same if using 32F.
With 16U, calculation of threshold loose precision and we might have small differences if numbers are rounded up or down.
=> Done, but no clear gain?


========================
### Determining Threshold

percentile might be derived from the histogram (so less data need to be stored)
but we then we sort all the pixels to figure out the pixel intensity at which we can cut off, that is extremely inefficient
since we need to sort billions of pixels in ascending order, and stored the result.

=> using an higher level of pyramid is not working since intensity is averaged. It is complicated to figure out exactly the loss.
=> a sampling approach on a small subset of data seems to work well.


========================
### Merging blob is slow

In Analyzer, We could collapse the pixels to do less comparions later?
Very little gain

In merge, we should look at each pixel in the list of coordinates and if it is in the blob we just merged, we should delete them.

++++++++++++++++

Improvements

Threshold finder
    Sampling should gives good results.



===================
Usage

Previous : Online : 60s anf 6GB for a 800MB image

The phase of discovery where we find the right parameters can be done at an higher pyramid level.
For our test data a 800MB image, which takes 27s at level 0, takes 7 sec at level 1 and 2s at level 2.
So running at level 2 gives us essentially an online algo (takes 300MB of memory).
Once parameters are found, segmentation can be run at level 0 again -> (27s / 600MB).

