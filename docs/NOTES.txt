=======================
### Resolutions

Different resolutions can be asked for.

In the Java code, threshold computation is always using 32F.
With 16U, calculation of threshold loose precision and we might have small differences\
if numbers are rounded up or down but really it does not make a difference.
=> We can choose the resolution as a parameter.
No clear gain in exec time but definitely saving space.


========================
### Determining Threshold

percentile might be derived from the histogram (so less data need to be stored)
but we then we sort all the pixels to figure out the pixel intensity at which we can cut off, that is extremely inefficient
since we need to sort billions of pixels in ascending order, and stored the result.

=> using an higher level of pyramid is not working since intensity is averaged. It is complicated to figure out exactly the loss.
=> a sampling approach on a small subset of data seems to work good enough but we loose determnism of course.

It has been suggested to use the histogram, which could work up to 16bits (we just store 65536 values), for 32bits, it becomes
unpractical and we have to go back to the previous method (20000 * 20000 image 32bits is => 400M * 4 => 1.6GB vs 2^32=4GB)

=====================
EROSION ON FEATURE TILE RATHER THAN FULL FEATURES
Two approach :
- read from disk, rewrite each as a mask, apply erosion, write bitmask back => a lot of extra computation + IO
- have a loader of feature bitmask in fast image. Unpack tile, erode, copy back the modified bitmask
(but not necessarily a tight bounding box anymore + feature can be split in two).

Better : rewrite custom erosion on feature.
Any other treatment that would require only the bitmask? Probably not. Only feature extraction that has to do with shape.

=> Solution : We now have a loader for feature bitmask. We apply openCV erosion and copy back.
Two remaining problems :
- the bound can be less tight (we cannot expect to find a feature pixel on each border)
- the area is still valid but a feature can become split into several pieces if the erosion removes a tiny bridge between them.

If we want to solve the second problem, we would need to run connected component again on it.
And then decide what to do with each member. Do we need to create new feature from existing feature if it split, etc...
That becomes much more involved for marginal gains (matching the global algorithm more closely) so for now we stick to this implementation.

====================
Current Improvements

Threshold finder
    Sampling should gives good results and cut memory consumption

Blobs are now compacted into Feature before being sent to merge.
    It cuts down on memory consumption by a factor of 64. (2 32bits coord vs 1bit per pixel)


===================
Usage

Previous : Online : 60s anf 6GB for a 800MB image

The phase of discovery where we find the right parameters can be done at an higher pyramid level.
For our test data a 800MB image, which takes 27s at level 0, takes 7 sec at level 1 and 2s at level 2.
So running at level 2 gives us essentially an online algo (takes 300MB of memory).
Once parameters are found, segmentation can be run at level 0 again -> (27s / 600MB).


===================
TODO

Improvement : accelerate.
 In merge, we should look at each pixel in the list of coordinates and if it is in the blob we just merged, we should delete them.

Parameterize erosion with different erosion

-------------------




=====================
How this work transform the original workflow?


Optimization problem
We could discard the holes beyond a certain size, deeming there are background for sure.
This could prevent keeping around a lot background pixels.
However, on the flip side, note that this implies that later in the merge stage
we will potentially try to merge contiguous "holes" to this missing background.
Then those holes could be seen as too small and removed, creating bridges between two blobs when it should not.
And since we do not expect this bad result, we could generate a labeled mask
with what seem one object but labeled as several objects.


==================
Things to look at

For dataset test2, two background blobs are found not be linked even though it look like they are connected in the final mask


================================
Optimizations

Resolution
- Resolution adapts to the input. We won't manipulate 32bits floating point encoded gradient if the input image is 8bits.
- We adapt the labeled mask resolution to the number of features represented. Ex: if we have 120 features, we will only generate an 8bit mask.

Thresholdfinder
 - limit the number of operations, duplication of data structures etc....
 - sampling the image to calculate the histogram and the gradient pixel threshold intensity.
 - possibility to use a higher resolution if using a pyramid (does not seem to lead to valid result)

Intensity Threshold values :
 - Bounds can also be computed at a higher level of the pyramid.

Local treatment for mask generation with a merging step.



