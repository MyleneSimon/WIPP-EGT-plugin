=======================
### Resolutions

Different resolutions can be asked for.

For a 16U image,
Threshold computation is always the same if using 32F.
With 16U, calculation of threshold loose precision and we might have small differences if numbers are rounded up or down.
=> Done, but no clear gain?


========================
### Determining Threshold

percentile might be derived from the histogram (so less data need to be stored)
but we then we sort all the pixels to figure out the pixel intensity at which we can cut off, that is extremely inefficient
since we need to sort billions of pixels in ascending order, and stored the result.

=> using an higher level of pyramid is not working since intensity is averaged. It is complicated to figure out exactly the loss.
=> a sampling approach on a small subset of data seems to work well.

DONE

====================
Current Improvements

Threshold finder
    Sampling should gives good results and cut memory consumption even by keeping the current technique

Blobs are now compacted into Feature before being sent to merge.
    It cuts down on memory consumption by a factor of 64. (2 32bits coord vs 1bit per pixel)



===================
Usage

Previous : Online : 60s anf 6GB for a 800MB image

The phase of discovery where we find the right parameters can be done at an higher pyramid level.
For our test data a 800MB image, which takes 27s at level 0, takes 7 sec at level 1 and 2s at level 2.
So running at level 2 gives us essentially an online algo (takes 300MB of memory).
Once parameters are found, segmentation can be run at level 0 again -> (27s / 600MB).


===================
TODO

Improvement : accelerate.
 In merge, we should look at each pixel in the list of coordinates and if it is in the blob we just merged, we should delete them.

----------
After erosion, size must be recalculated to remove objects that became too small.
This means refactoring feature so the number of pixels is readily available.

----------

Connectivity 8 for analyser

----------

Parameterize erosion



=====================
How this work transform the original workflow?

We have to review the workflow if we want to be efficient.
Example : in the original code we do an erosion step before generating the mask.
We cannot do that anymore : we generate feature early on that we merge. They describe the number of pixels for each feature.
If we do erosion, we need to:
 - 1/ generate the mask data from the feature
 - 2/generate apply erosion
 - 3/write the mask
 - 4/regenerate the feature collection if we need them (so the values can be updated).


Optimization problem
We could discard the holes beyond a certain size, deeming there are background for sure.
This could prevent keeping around a lot background pixels.
However, on the flip side, note that this implies that later in the merge stage
we will potentially try to merge contiguous "holes" to this missing background.
Then those holes could be seen as too small and removed, creating bridges between two blobs when it should not.
And since we do not expect this bad result, we could generate a labeled mask
with what seem one object but labeled as several objects.


==================
Things to look at

For dataset test2, two background blobs are found not be linked even though it look like they are connected in the final mask