=======================
### Resolutions

Different resolutions can be asked for.

For a 16U image,
Threshold computation is always the same if using 32F.
With 16U, calculation of threshold loose precision and we might have small differences if numbers are rounded up or down.
=> Done, but no clear gain?


========================
### Determining Threshold

percentile might be derived from the histogram (so less data need to be stored)
but we then we sort all the pixels to figure out the pixel intensity at which we can cut off, that is extremely inefficient
since we need to sort billions of pixels in ascending order, and stored the result.

=> using an higher level of pyramid is not working since intensity is averaged. It is complicated to figure out exactly the loss.
=> a sampling approach on a small subset of data seems to work well.

DONE

====================
Current Improvements

Threshold finder
    Sampling should gives good results and cut memory consumption even by keeping the current technique

Blobs are now compacted into Feature before being sent to merge.
    It cuts down on memory consumption by a factor of 64. (2 32bits coord vs 1bit per pixel)



===================
Usage

Previous : Online : 60s anf 6GB for a 800MB image

The phase of discovery where we find the right parameters can be done at an higher pyramid level.
For our test data a 800MB image, which takes 27s at level 0, takes 7 sec at level 1 and 2s at level 2.
So running at level 2 gives us essentially an online algo (takes 300MB of memory).
Once parameters are found, segmentation can be run at level 0 again -> (27s / 600MB).


===================
TODO

Improvement : accelerate.
 In merge, we should look at each pixel in the list of coordinates and if it is in the blob we just merged, we should delete them.

Connectivity 8 for analyser



=====================
Problems

We have to review the workflow if we want to be efficient.
Example : in the original code we do an erosion step before generating the mask.
We cannot do that anymore : we generate feature early on that we merge. They describe the number of pixels for each feature.
If we do erosion, we need to:
 - 1/ generate the mask data from the feature
 - 2/generate apply erosion
 - 3/write the mask
 - 4/regenerate the feature collection if we need them (so the values can be updated).

